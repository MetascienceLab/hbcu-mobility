{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f0f992-e77c-47d7-b462-9263b35704ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load header and path to ANI\n",
    "permissions = \"fulldata\"  # default or fulldata (depends which cluster you are on)\n",
    "# set the project name (you receive this via email)\n",
    "project_name = \"hbcu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9383da3e-b65c-4330-ba47-5552f5a402e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT DETAILS:\n",
      "Project identifier: hbcu\n",
      "Cluster data access level: fulldata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTION DETAILS:\n",
      "Spark version: 11.3.x-scala2.12, \n",
      "Cluster Node Type: i3.xlarge, \n",
      "Driver Node Type: c4.2xlarge, \n",
      "Worker Node Type: i3.xlarge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SNAPSHOT DETAILS:\n",
      "This notebook uses the latest snapshot date for this version: \"v008.20231003094404/\", (03 October 2023)\n",
      "\n",
      " The following are the Scopus snapshot dates available for datasets at v008:\n",
      "['v008.20220314092239/', 'v008.20220906063204/', 'v008.20221021151538/', 'v008.20230509104116/', 'v008.20230821105000/', 'v008.20231003094404/']\n",
      "(If you want to use a different snapshot, in the first cell of your notebook, set the variable `custom_snapshot_date` to one of the options listed above (string data type))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SAMPLE DETAILS:\n",
      "Setting paths to run on full datasets (100% Scopus publication volume)\n"
     ]
    }
   ],
   "source": [
    "%run /Snippets/header_008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba04ec0-9ab9-41e0-9f1d-8baa0f5fd9fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import stats\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37231026-b0c7-4896-8e54-ab09210da810",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['full_hbcu.csv', 'hbcu_au_eid.csv', 'hbcu_au_eid_match.csv', 'hbcu_cd_pct.csv', 'hbcu_cd_pct_match.csv', 'hbcu_citepct.csv', 'hbcu_citepct_match.csv', 'hbcu_collab_af.csv', 'hbcu_collab_afnum.csv', 'hbcu_collab_fresh.csv', 'hbcu_collab_senior.csv', 'hbcu_doi.csv', 'hbcu_eid.csv', 'hbcu_eid_match.csv', 'hbcu_match_doi.csv', 'hbcu_match_name.csv', 'hbcu_multiinst_info.csv', 'hbcu_novel.csv', 'hbcu_novel_match.csv', 'hbcu_person_name.csv', 'multi_inst_disambiguate.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the list of all files and directories\n",
    "path = \"/dbfs/path/hbcu\"\n",
    "dir_list = os.listdir(path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad355cc6-c0fe-418e-8081-92b952df5b79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.rm(\"dbfs:/path/hbcu/hbcu_match_name.csv\",True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75318d46-8f89-494b-b336-6f3159950389",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cmd 2 gives us locations\n",
    "df_ani = spark.read.format(\"parquet\").load(basePath+tablename_ani)\n",
    "df_doi = (spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "              .load('dbfs:/path/hbcu/hbcu_match_doi.csv') #hbcu_doi.csv\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561be7b4-fb24-44c8-8b55-d48d939328bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# match DOI to eid\n",
    "df_doi_eid = (\n",
    "    df_doi\n",
    "    # normalize doi\n",
    "    .withColumn(\"doi\", f.trim(f.lower(f.col(\"doi\"))))\n",
    "    .join(\n",
    "        df_ani.select(\"doi\", \"eid\", \"year\").withColumn(\n",
    "            \"doi\", f.trim(f.lower(f.col(\"doi\")))\n",
    "        ),\n",
    "        [\"doi\"],\n",
    "        \"inner\",\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "# display(df_doi_eid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58417f9e-d7af-491c-b277-a57a8c69d4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Get auid for every person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbe45bb-a270-43dc-b752-b14175d6acdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_name = (spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "              .load('dbfs:/path/hbcu/hbcu_match_name.csv') #hbcu_person_name.csv\n",
    "          )\n",
    "\n",
    "#display(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b961ad67-3765-4725-88a3-ac724341af17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ani_au = (\n",
    "    df_ani\n",
    "    # get final author position\n",
    "    .withColumn(\"last_author_seq\", f.array_max(f.col(\"Au.Authorseq\")))\n",
    "    .withColumn(\"Au\", f.explode(\"Au\"))\n",
    "    .select(\n",
    "        \"eid\",\n",
    "      'year',\n",
    "        \"Au.auid\",\n",
    "        \"Au.initials\",\n",
    "        \"Au.surname\",\n",
    "        \"Au.Authorseq\",\n",
    "        \"last_author_seq\",\n",
    "      \"publication_type\", 'Au_Af', 'Af.affiliation_ids'\n",
    "    )\n",
    ")\n",
    "\n",
    "# set mapping between PersonId_control and auid\n",
    "t = (\n",
    "    df_doi_eid.join(df_name, [\"PersonId_control\"])\n",
    "    .withColumn(\"first_intial\", f.col(\"firstname\").substr(1, 1)).select(\"PersonId_control\", \"lastname\", \"firstname\", \"first_intial\", \"eid\")\n",
    "    .join(\n",
    "        df_ani_au.select(\"eid\", \"auid\", \"initials\", \"surname\", \"Authorseq\", \"last_author_seq\",),\n",
    "        [\"eid\"],\n",
    "        \"inner\",\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "\n",
    "df_auid = (\n",
    "    t\n",
    "    # normalize names\n",
    "    .withColumn(\"first_intial\", f.trim(f.lower(f.col(\"first_intial\"))))\n",
    "    .withColumn(\n",
    "        \"lastname\",\n",
    "        f.trim(f.lower(f.regexp_replace(f.col(\"lastname\"), \"[^a-zA-Z]\", \"\"))),\n",
    "    )\n",
    "    .withColumn(\"scopus_intial\", f.trim(f.lower(f.col(\"initials\"))))\n",
    "    .withColumn(\n",
    "        \"scopus_lastname\",\n",
    "        f.trim(f.lower(f.regexp_replace(f.col(\"surname\"), \"[^a-zA-Z]\", \"\"))),\n",
    "    )\n",
    "    # find equal names; many authors don't have given names, use initial instead\n",
    "    .filter(\n",
    "        (f.col(\"lastname\") == f.col(\"scopus_lastname\"))\n",
    "        & (f.col(\"scopus_intial\").contains(f.col(\"first_intial\")))\n",
    "    )\n",
    "    # get the final mapping\n",
    "    .select(\"PersonId_control\", \"auid\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# why some pub authors don't have match?\n",
    "# e.g., TAKACS HAYNES - Haynes; MARTINEZ-LOPEZ - Martínez-López\n",
    "\n",
    "df_auid_plus = (\n",
    "    t.join(\n",
    "        df_auid.select(\"PersonId_control\").withColumn(\"ind\", f.lit(1)).drop_duplicates(),\n",
    "        [\"PersonId_control\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .filter(\"ind is null\")\n",
    "    # replace special letters\n",
    "    .withColumn(\"clean_lastname\", f.lower(f.regexp_replace(\"lastname\", \"-\", \" \")))\n",
    "    .withColumn(\"clean_scopus_lastname\", f.lower(f.regexp_replace(\"surname\", \"-\", \" \")))\n",
    "    .withColumn(\n",
    "        \"clean_lastname\",\n",
    "        f.translate(f.col(\"clean_lastname\"), \"áàâäãåąÁÀÂÄÃÅĄ\", \"a\" * 14),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_lastname\", f.translate(f.col(\"clean_lastname\"), \"éèêëęÉÈÊËĘ\", \"e\" * 10)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_lastname\", f.translate(f.col(\"clean_lastname\"), \"íìîïÍÌÎÏ\", \"i\" * 8)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_lastname\", f.translate(f.col(\"clean_lastname\"), \"óòôöõøÓÒÔÖÕØ\", \"o\" * 12)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_lastname\", f.translate(f.col(\"clean_lastname\"), \"úùûüÚÙÛÜ\", \"u\" * 8)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_lastname\",\n",
    "        f.translate(\n",
    "            f.col(\"clean_lastname\"),\n",
    "            \"ćçğłńñřșşśšțýźžĆÇĞŁÑŃŘȘŞŚŠȚÝŹŽ\",\n",
    "            \"ccglnnrsssstyzzCCGLNNRSSSSTYZZ\",\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(f.col(\"clean_scopus_lastname\"), \"áàâäãåąÁÀÂÄÃÅĄ\", \"a\" * 14),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(f.col(\"clean_scopus_lastname\"), \"éèêëęÉÈÊËĘ\", \"e\" * 10),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(f.col(\"clean_scopus_lastname\"), \"íìîïÍÌÎÏ\", \"i\" * 8),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(f.col(\"clean_scopus_lastname\"), \"óòôöõøÓÒÔÖÕØ\", \"o\" * 12),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(f.col(\"clean_scopus_lastname\"), \"úùûüÚÙÛÜ\", \"u\" * 8),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"clean_scopus_lastname\",\n",
    "        f.translate(\n",
    "            f.col(\"clean_scopus_lastname\"),\n",
    "            \"ćçğłńñřșşśšțýźžĆÇĞŁÑŃŘȘŞŚŠȚÝŹŽ\",\n",
    "            \"ccglnnrsssstyzzCCGLNNRSSSSTYZZ\",\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\"first_intial\", f.trim(f.lower(f.col(\"first_intial\"))))\n",
    "    .withColumn(\"scopus_intial\", f.trim(f.lower(f.col(\"initials\"))))\n",
    "    # split multi-word last names\n",
    "    .withColumn(\"lastname_list\", f.split(\"clean_lastname\", \" \"))\n",
    "    .withColumn(\"scopus_lastname_list\", f.split(\"clean_scopus_lastname\", \" \"))\n",
    "    # Determine the common words in both last name lists\n",
    "    .withColumn(\n",
    "        \"common_words\", f.array_intersect(\"lastname_list\", \"scopus_lastname_list\")\n",
    "    )\n",
    "    .filter(\n",
    "        (f.size(\"common_words\") >= 1)\n",
    "        & (f.col(\"scopus_intial\").contains(f.col(\"first_intial\")))\n",
    "    )\n",
    "    # get the final mapping\n",
    "    .select(\"PersonId_control\", \"auid\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "df_auid = df_auid.unionByName(df_auid_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa871702-9ac5-4935-9d09-e6dde2cd7c66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reconstruct their pub list\n",
    "this is an author level feature table / author-eid mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbdb415-9f1e-4531-9aec-c1f36ddf556e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_publist = (\n",
    "    df_ani_au\n",
    "    # filter paper type and pubyear\n",
    "    .filter(\"publication_type in ('ar', 'cp', 're')\")\n",
    "    .filter(\"year >= 2006\")\n",
    "    .drop(\"initials\", \"surname\")\n",
    "    .join(df_auid, [\"auid\"])\n",
    ")\n",
    "\n",
    "df_publist = (\n",
    "    df_publist\n",
    "    # only keep pubs in the AA data before 2020; keep the rest\n",
    "    .join(\n",
    "        df_doi_eid.select(\"eid\", \"PersonId_control\", \"doi\").withColumn(\"isAApub\", f.lit(1)),\n",
    "        [\"eid\", \"PersonId_control\"],\n",
    "        \"outer\",\n",
    "    )\n",
    "    # exclude super author article\n",
    "    .filter(f.col(\"last_author_seq\") < 100).drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a6914a-97a9-44b5-96d9-11b40ac2de3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter Scopus appended new pubs by aff\n",
    "# t = df_publist.filter('isAApub = 1').select('eid', 'PersonId_control').join(df_ani.select('eid', 'Au_Af'), ['eid'], 'left').drop_duplicates()\n",
    "# display(t.filter(\"Au_Af is null\").count() / t.count())\n",
    "\n",
    "t = (\n",
    "    df_publist.withColumn(\"auaf\", func.explode(\"Au_Af\"))\n",
    "    .filter(\"Authorseq=auaf.Authorseq\")\n",
    "    .withColumn(\"affiliation\", func.expr(\"affiliation_ids[auaf.affiliation_seq-1]\"))\n",
    "    .withColumn(\"affiliation\", f.explode(\"affiliation\"))\n",
    ")\n",
    "\n",
    "tScopus = (\n",
    "    df_publist.filter(\"isAApub is null\")\n",
    "    .withColumn(\"affiliation\", f.explode(f.flatten(\"affiliation_ids\")))\n",
    "    .join(\n",
    "        t.filter(\"isAApub = 1\").select(\n",
    "            \"PersonId_control\",\n",
    "            \"affiliation\",\n",
    "        ),\n",
    "        [\"PersonId_control\", \"affiliation\"],\n",
    "    )\n",
    "    .drop(\"Au_Af\", \"auaf\", \"affiliation\", \"affiliation_ids\")\n",
    ")\n",
    "\n",
    "df_publist = (\n",
    "    df_publist.filter(\"isAApub = 1\")\n",
    "    .drop(\"Au_Af\", \"affiliation_ids\")\n",
    "    .unionByName(tScopus)\n",
    "    .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1b07b9-30fc-48d7-98e2-e07986fb6ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# append Author gender by Scopus\n",
    "#df_gender = spark.read.format(\"parquet\").load(basePath + tablename_gender_inference)\n",
    "# df_publist = df_publist.join(\n",
    "#     df_gender.select(\"auid\", f.col(\"Inferred_Probable_Gender\").alias(\"scopus_gender\")),\n",
    "#     [\"auid\"],\n",
    "#     \"left\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94aae4c6-83b8-418d-ba28-181e2fbc98bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "df_export = df_publist.toPandas()\n",
    "df_export.to_csv('/dbfs/path/hbcu/hbcu_au_eid_match.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3d2cb5-2162-4e69-a9a6-4a5cee8f0f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a423f81-91a3-4327-a16f-fab4ecf9b14b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "calculate all publications' citations. Rank all pub's citations by year & subfield. Get percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c1b29-a457-45a1-8c42-7b4d88368122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get each paper's citations in all dataset\n",
    "df_ani_t = (\n",
    "    df_ani\n",
    "    # concat date\n",
    "    .withColumn(\n",
    "        \"time\",\n",
    "        f.to_date(f.concat_ws(\"\", f.col(\"year\"), f.col(\"date_sort_month\")), \"yyyyMM\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_cite_ani = (\n",
    "    df_ani_t.select(\n",
    "        f.col(\"eid\").alias(\"citing_eid\"),\n",
    "        f.col(\"time\").alias(\"citing_time\"),\n",
    "        \"references\",\n",
    "    )  # e.g. [438974823, 437082432, 8943021, 30482]\n",
    "    .withColumn(\"cited_eid\", f.explode(\"references\"))\n",
    "    .drop(\"references\")\n",
    "    # use ani to get citation year\n",
    "    .join(\n",
    "        df_ani_t\n",
    "        # romove cited papers that don't fit the standard\n",
    "        .filter(\"publication_type in ('ar', 'cp', 're')\")\n",
    "        .filter(\"year >= 2006\")\n",
    "        .select(\n",
    "            [\n",
    "                \"EID\",\n",
    "                f.col(\"time\").alias(\"cited_time\"),\n",
    "            ]\n",
    "        ).alias(\"t1\"),\n",
    "        f.col(\"t1.EID\") == f.col(\"cited_eid\"),\n",
    "        \"inner\",\n",
    "    )\n",
    "    .drop(\"EID\")\n",
    "    # set year fit logic\n",
    "    .filter((f.col(\"citing_time\") >= f.col(\"cited_time\")))\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# exclude self citation\n",
    "# get each article's first and last author, if any is same, it's self-citation\n",
    "# first author list\n",
    "t1 = df_ani_au.filter(\"Authorseq = 1\").drop_duplicates()\n",
    "# last\n",
    "tlast = (\n",
    "    df_ani_au.filter(\"Authorseq = last_author_seq\")\n",
    "    .select(\"eid\", \"auid\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_cite_ani_noself = (\n",
    "    df_cite_ani\n",
    "    # append first\n",
    "    .join(\n",
    "        t1.select(f.col(\"eid\").alias(\"cited_eid\"), f.col(\"auid\").alias(\"cited_1_auid\")),\n",
    "        [\"cited_eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .join(\n",
    "        t1.select(\n",
    "            f.col(\"eid\").alias(\"citing_eid\"), f.col(\"auid\").alias(\"citing_1_auid\")\n",
    "        ),\n",
    "        [\"citing_eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .filter(\"cited_1_auid != citing_1_auid\")\n",
    "    # append last\n",
    "    .join(\n",
    "        tlast.select(\n",
    "            f.col(\"eid\").alias(\"cited_eid\"), f.col(\"auid\").alias(\"cited_last_auid\")\n",
    "        ),\n",
    "        [\"cited_eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .join(\n",
    "        tlast.select(\n",
    "            f.col(\"eid\").alias(\"citing_eid\"), f.col(\"auid\").alias(\"citing_last_auid\")\n",
    "        ),\n",
    "        [\"citing_eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .filter(\"cited_last_auid != citing_last_auid\")\n",
    "    .select(\"citing_eid\", \"cited_eid\", \"citing_time\", \"cited_time\")\n",
    ")\n",
    "\n",
    "# display(df_cite_ani_noself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f4bed7-607c-43c3-b6da-a8466d576147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use SM to find subfield average citations as denom\n",
    "df_smc_ani = spark.read.format(\"parquet\").load(basePath + tablename_smc_complete)\n",
    "\n",
    "# for every paper after 2006, get its SM field\n",
    "# add SM fields; hybrid first, then match journal\n",
    "\n",
    "# use hybrid\n",
    "df_ani_sm = df_ani.join(\n",
    "    df_smc_ani.select(\"eid\", \"subfield_hybrid\"), [\"eid\"], \"left\"\n",
    ").select(\"eid\", \"subfield_hybrid\")\n",
    "\n",
    "# find unmatched\n",
    "t_na = df_ani_sm.filter(f.col(\"subfield_hybrid\").isNull())\n",
    "if t_na.count() > 0:\n",
    "    t2 = (\n",
    "        t_na.join(df_ani.select(\"eid\", \"source.srcid\"), [\"eid\"], \"inner\")\n",
    "        .join(\n",
    "            df_smc_ani.select(\"srcid\", \"subfield_journal\").drop_duplicates(),\n",
    "            [\"srcid\"],\n",
    "            \"inner\",\n",
    "        )\n",
    "        .drop(\"subfield_hybrid\")\n",
    "        .withColumnRenamed(\"subfield_journal\", \"subfield_hybrid\")\n",
    "        .select(\"eid\", \"subfield_hybrid\")\n",
    "    )\n",
    "    df_ani_sm = df_ani_sm.dropna().unionByName(t2).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df667d82-ec4f-4923-818c-69abd7e38555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get each paper's citation: 3-year, 5-year\n",
    "df_cite_metric3 = (\n",
    "    df_cite_ani_noself.filter(\n",
    "        f.col(\"citing_time\") <= f.add_months(f.col(\"cited_time\"), 12 * 3)\n",
    "    )\n",
    "    .groupby(\"cited_eid\")\n",
    "    .agg(f.countDistinct(\"citing_eid\").alias(\"cite3\"))\n",
    "    .withColumnRenamed(\"cited_eid\", \"eid\")\n",
    ")\n",
    "\n",
    "df_cite_metric5 = (\n",
    "    df_cite_ani_noself.filter(\n",
    "        f.col(\"citing_time\") <= f.add_months(f.col(\"cited_time\"), 12 * 5)\n",
    "    )\n",
    "    .groupby(\"cited_eid\")\n",
    "    .agg(f.countDistinct(\"citing_eid\").alias(\"cite5\"))\n",
    "    .withColumnRenamed(\"cited_eid\", \"eid\")\n",
    ")\n",
    "\n",
    "w3 = Window.partitionBy(\"subfield_hybrid\", \"year\").orderBy(\"cite3\")\n",
    "w5 = Window.partitionBy(\"subfield_hybrid\", \"year\").orderBy(\"cite5\")\n",
    "\n",
    "df_cite_metric = (\n",
    "    df_ani_t\n",
    "    # romove cited papers that don't fit the standard\n",
    "    .filter(\"publication_type in ('ar', 'cp', 're')\")\n",
    "    .filter(\"year >= 2006\")\n",
    "    .join(\n",
    "        df_ani_sm.select(\"eid\", \"subfield_hybrid\"),\n",
    "        [\"eid\"],\n",
    "        \"inner\",\n",
    "    )\n",
    "    .join(df_cite_metric3, [\"eid\"], \"left\")\n",
    "    .join(df_cite_metric5, [\"eid\"], \"left\")\n",
    "    .select(\"eid\", \"subfield_hybrid\", \"cite3\", \"cite5\", 'year')\n",
    "    .fillna(0)\n",
    "    .drop_duplicates()\n",
    "    # add percentile for every paper by subfield and year\n",
    "    .withColumn(\"cite3_pct\", f.percent_rank().over(w3))\n",
    "    .withColumn(\"cite5_pct\", f.percent_rank().over(w5))\n",
    ")\n",
    "\n",
    "# merge to publist\n",
    "df_eid_citepct = (\n",
    "    df_publist.select(\"eid\", \"year\")\n",
    "    .join(df_cite_metric, [\"eid\", \"year\"], \"left\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "df_export = df_eid_citepct.toPandas()\n",
    "df_export.to_csv('/dbfs/path/hbcu/hbcu_citepct_match.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f620fb8-59cb-42ee-92c0-ce59e99e16be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Average relative citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f98e0f-e679-4be9-ad41-1843b992c31d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a df containing all cited paper's field\n",
    "df_cited_field = df_cite_ani.join(\n",
    "    df_ani_sm.select(\"eid\", \"subfield_hybrid\"),\n",
    "    f.col(\"eid\") == f.col(\"cited_eid\"),\n",
    "    \"inner\",\n",
    ")\n",
    "\n",
    "# get 3-year total citations per year\n",
    "t3 = (\n",
    "    df_cited_field.filter(f.col(\"citing_time\") <= f.add_months(f.col(\"cited_time\"), 36))\n",
    "    .groupby(\"subfield_hybrid\", f.year(f.col(\"cited_time\")).alias(\"year\"))\n",
    "    .agg(f.countDistinct(\"citing_eid\").alias(\"sum_cite_3yr\"))\n",
    ")\n",
    "\n",
    "# get 5-year total citations per year\n",
    "t5 = (\n",
    "    df_cited_field.filter(f.col(\"citing_time\") <= f.add_months(f.col(\"cited_time\"), 60))\n",
    "    .groupby(\"subfield_hybrid\", f.year(f.col(\"cited_time\")).alias(\"year\"))\n",
    "    .agg(f.countDistinct(\"citing_eid\").alias(\"sum_cite_5yr\"))\n",
    ")\n",
    "\n",
    "# count denom; contains 0 citation papers\n",
    "denom = (\n",
    "    df_ani.select(\"eid\", \"year\")\n",
    "    .join(df_ani_sm.select(\"eid\", \"subfield_hybrid\"), [\"eid\"], \"inner\")\n",
    "    .groupby(\"subfield_hybrid\", \"year\")\n",
    "    .agg(f.countDistinct(\"eid\").alias(\"denom_eid_num\"))\n",
    ")\n",
    "\n",
    "df_baseline_cite = (\n",
    "    t3\n",
    "    .join(t5, [\"subfield_hybrid\", \"year\"], \"outer\")\n",
    "    .join(denom, [\"subfield_hybrid\", \"year\"], \"left\")\n",
    "    .withColumn(\"mean_cite_3yr\", f.col(\"sum_cite_3yr\") / f.col(\"denom_eid_num\"))\n",
    "    .withColumn(\"mean_cite_5yr\", f.col(\"sum_cite_5yr\") / f.col(\"denom_eid_num\"))\n",
    "    .select(\n",
    "        \"subfield_hybrid\", \"year\", \"mean_cite_3yr\", \"mean_cite_5yr\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# display(df_baseline_cite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdba455f-418c-4ab8-91de-ea8cadab4b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sources schema for SJR and SNIP\n",
    "df_sources = spark.read.format(\"parquet\").load(basePath + tablename_sources)\n",
    "df_sources = (\n",
    "    df_sources.filter(\"isActive == 'true'\")\n",
    "    .withColumn(\"metrics_calculations\", f.explode(\"metrics_calculations\"))\n",
    "    .select(\n",
    "        [\n",
    "            \"srcid\",\n",
    "            \"metrics_calculations.year\",\n",
    "            \"metrics_calculations.SJR\",\n",
    "            \"metrics_calculations.SNIP\",\n",
    "        ]\n",
    "    )\n",
    "    .filter(\"year >=2006\")\n",
    ")\n",
    "\n",
    "df_cite_count = (\n",
    "    df_publist\n",
    "    .join(df_cite_metric, [\"eid\"], \"left\")\n",
    "    # add year, srcid\n",
    "    .join(\n",
    "        df_ani[['eid', \"source.srcid\", \"year\"]],\n",
    "        [\"eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .join(\n",
    "        df_ani_sm.select(\n",
    "            \"eid\", \"subfield_hybrid\"\n",
    "        ).drop_duplicates(),\n",
    "        [\"eid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .join(df_baseline_cite, [\"subfield_hybrid\", \"year\"], \"left\")\n",
    "    # calculate ARC\n",
    "    .withColumn(\"arc_3yr_noself\", f.col(\"cite3\") / f.col(\"mean_cite_3yr\"))\n",
    "    .withColumn(\"arc_5yr_noself\", f.col(\"cite5\") / f.col(\"mean_cite_5yr\"))\n",
    "    # Journal prestige (SJR, SNIP)\n",
    "    .join(df_sources.select(\"srcid\", \"year\", \"SJR\", \"SNIP\"), [\"srcid\", \"year\"], \"left\")\n",
    "    .drop(\"mean_cite_3yr\", \"mean_cite_5yr\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "# display(df_cite_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89f5332-3374-45fe-8e38-5bf0a91cbc86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# citing year/month, doc type, # authors, # refs\n",
    "df_title = df_ani_t.select(\n",
    "    \"eid\", \"time\", \"publication_type\", f.size(\"Au\").alias(\"n_au\"), \"n_references\"\n",
    ")\n",
    "\n",
    "# display(df_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fcd422-8539-4fd2-aa54-d34228fb53b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_past_prod = (\n",
    "    df_publist.select(\"eid\")\n",
    "    .join(\n",
    "        df_ani_au.filter(\"Authorseq = 1 or Authorseq = last_author_seq\").select(\n",
    "            \"eid\", \"auid\", \"year\"\n",
    "        ),\n",
    "        [\"eid\"],\n",
    "    )\n",
    "    .join(\n",
    "        df_ani_au\n",
    "        # only count article, conf, review\n",
    "        .filter(\"publication_type in ('ar', 'cp', 're')\").select(\n",
    "            f.col(\"eid\").alias(\"past_eid\"),\n",
    "            f.col(\"auid\").alias(\"auid2\"),\n",
    "            f.col(\"year\").alias(\"past_year\"),\n",
    "        ),\n",
    "        ((f.col(\"auid\") == f.col(\"auid2\")) & (f.col(\"year\") > f.col(\"past_year\"))),\n",
    "    )\n",
    "    .groupby(\"eid\", \"auid\")\n",
    "    .agg(f.countDistinct(\"past_eid\").alias(\"num_past_eid\"))\n",
    "    .groupby(\"eid\")\n",
    "    .agg(f.mean(\"num_past_eid\").alias(\"avgnum_past_eid\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168011cd-e0db-4615-9be1-c324a5aa3e7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# eid level data\n",
    "df_eid_basic = (\n",
    "    df_publist.select(\"eid\", \"PersonId_control\")\n",
    "    .join(df_cite_count.withColumnRenamed(\"cited_eid\", \"eid\"), [\"eid\"], \"outer\")\n",
    "    .join(df_past_prod, [\"eid\"], \"left\")\n",
    "    .join(df_title, [\"eid\"], \"left\")\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# export 2\n",
    "df_export = df_eid_basic.toPandas()\n",
    "df_export.to_csv('/dbfs/path/hbcu/hbcu_eid_match.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec2c736-df42-46fe-b89a-a070be21c56b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Disruption metric (CD index)\n",
    "https://link.springer.com/article/10.1007/s11192-023-04644-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cc5a87-baca-437f-8d19-e893fac71284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 1280137\n",
      "2007 1444434\n",
      "2008 1601573\n",
      "2009 1760327\n",
      "2010 1880480\n",
      "2011 2034645\n",
      "2012 2146010\n",
      "2013 2257428\n",
      "2014 2349333\n",
      "2015 2386661\n",
      "2016 2494264\n",
      "2017 2583831\n",
      "2018 2749014\n",
      "2019 2997639\n",
      "2020 3154400\n"
     ]
    }
   ],
   "source": [
    "df_eid = pd.read_csv(\n",
    "    r\"/dbfs/path/hbcu/hbcu_au_eid_match.csv\"\n",
    ")\n",
    "output = []\n",
    "\n",
    "for year in range(2006, 2021):\n",
    "  try:\n",
    "    df = pd.read_csv(r\"/dbfs/path/tenure_cd_{}.csv\".format(year))\n",
    "    output.append(df_eid[['eid']].merge(df[['eid','cd5','cd5_pct']], on='eid', how='inner'))\n",
    "    print(year, len(df))\n",
    "  except:\n",
    "    print(year, 'wrong')\n",
    "\n",
    "df_output = pd.concat(output).drop_duplicates()\n",
    "df_output.to_csv(r\"/dbfs/path/hbcu/hbcu_cd_pct_match.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bee441-11c4-446f-b35b-b835197ce8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Novelty/conventionality\n",
    "(10.1073/pnas.2118046119; https://www.science.org/doi/10.1126/science.1240474)\n",
    "https://www.sciencedirect.com/science/article/pii/S0048733314001826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3bd116-dca3-498a-b79d-35ab12dbcf87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 1383389\n",
      "2007 1468531\n",
      "2008 1554054\n",
      "2009 1704200\n",
      "2010 1801035\n",
      "2011 1940442\n",
      "2012 2035027\n",
      "2013 2132071\n",
      "2014 2262712\n",
      "2015 2303219\n",
      "2016 2337913\n",
      "2017 2421731\n",
      "2018 2574154\n",
      "2019 2801095\n",
      "2020 2937820\n"
     ]
    }
   ],
   "source": [
    "df_eid = pd.read_csv(\n",
    "    r\"/dbfs/path/hbcu/hbcu_au_eid_match.csv\"\n",
    ")\n",
    "output = []\n",
    "\n",
    "for year in range(2006, 2021):\n",
    "  try:\n",
    "    df = pd.read_csv(r\"/dbfs/path/tenure_novel_pct_{}.csv\".format(year))\n",
    "    output.append(df_eid[['eid']].merge(df[['eid','novelty','novelty_pct']], on='eid', how='inner'))\n",
    "    print(year, len(df))\n",
    "  except:\n",
    "    print(year, 'wrong')\n",
    "\n",
    "df_output = pd.concat(output).drop_duplicates()\n",
    "df_output.to_csv(r\"/dbfs/path/hbcu/hbcu_novel_match.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b23126-3e85-4b8d-8087-b514a2c3175c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Collaborator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85c0291-3276-4ab0-99ba-4e228f5a27cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# control version\n",
    "df_ipr = spark.read.format(\"parquet\").load(basePath + tablename_ipr)\n",
    "\n",
    "# get collaborator affiliation & country\n",
    "df_collab_af = (\n",
    "    df_publist\n",
    "    # remove single author\n",
    "    .filter(\"Authorseq > 1\")\n",
    "    .select(\"eid\")\n",
    "    .join(df_ani.select(\"eid\", \"Af.afid\"), [\"eid\"], \"inner\")\n",
    "    .withColumn(\"afid\", f.explode(\"afid\"))\n",
    "    .join(\n",
    "        df_ipr.select(\n",
    "            \"afid\", f.col(\"preferred_name\").alias(\"afname\"), \"state\", \"country\"\n",
    "        ),\n",
    "        [\"afid\"],\n",
    "        \"left\",\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "df_collab_af.toPandas().to_csv(f'/dbfs/path/hbcu/hbcu_collab_af_match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7c395c-14f2-4e2a-84d2-60df51bdf2de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_afnum = df_collab_af.groupby(\"eid\").agg(\n",
    "    # mark same-af papers\n",
    "    f.countDistinct(\"afid\").alias(\"af_num\"),\n",
    "    # mark same-country papers\n",
    "    f.countDistinct(\"country\").alias(\"ctry_num\"),\n",
    ")\n",
    "\n",
    "#display(df_afnum.groupby('af_num').agg(f.countDistinct('eid')))\n",
    "df_afnum.toPandas().to_csv(f'/dbfs/path/hbcu/hbcu_collab_afnum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe62dfa0-468c-421f-9a72-54ee5c4588e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get collaborator seniority\n",
    "\n",
    "df_senior = (\n",
    "    df_publist\n",
    "    .select(\"eid\", 'year', f.col('auid').alias('self_auid'))\n",
    "    .join(df_ani.withColumn(\"Au\", f.explode(\"Au\")).select(\"eid\", \"Au.auid\", 'Au.Authorseq'), [\"eid\"], \"inner\")\n",
    "    .filter('self_auid != auid')\n",
    "    .join(df_ani.select(\"Au.auid\", f.col('year').alias('collab_pubyear')).withColumn(\"auid\", f.explode(\"auid\")), [\"auid\"], \"inner\")\n",
    "    # get max publication length\n",
    "    .groupBy(\"eid\", 'auid', 'year', 'Authorseq').agg(f.min('collab_pubyear').alias('min_year'))\n",
    "    .withColumn('seniority', f.col('year') - f.col('min_year') + 1)\n",
    ")\n",
    "\n",
    "df_senior.toPandas().to_csv(f'/dbfs/path/hbcu/hbcu_collab_senior.csv')\n",
    "#display(df_senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f1521f-369a-454a-9f42-90e602f4e51a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# number new collaborators who did not appear in past five years\n",
    "\n",
    "df_publist_alltime = (\n",
    "    df_ani_au\n",
    "    # filter paper type and pubyear\n",
    "    .filter(\"publication_type in ('ar', 'cp', 're')\")\n",
    "    .drop(\"initials\", \"surname\")\n",
    "    .join(df_auid, [\"auid\"])\n",
    "    .select('eid', 'auid', 'year')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae763a6-78c8-452c-8406-0cc164045833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_au_eid = pd.read_csv(\n",
    "    r\"/dbfs/path/tenure_au_eid_1106.csv\"\n",
    ")\n",
    "df_au_eid = df_au_eid[[\"auid\", \"eid\", \"PersonId\", \"year\"]]\n",
    "df_au_eid = spark.createDataFrame(df_au_eid)\n",
    "\n",
    "# merge\n",
    "df_compare1 = df_au_eid.join(\n",
    "    df_au_eid.select(\n",
    "        f.col(\"PersonId\"),\n",
    "        f.col(\"eid\").alias(\"past_eid\"),\n",
    "        f.col(\"year\").alias(\"past_year\"),\n",
    "    ),\n",
    "    [\"PersonId\"],\n",
    ").filter(\"year - past_year <= 5 and year - past_year > 0 and eid != past_eid\")\n",
    "df_compare2 = df_au_eid.join(\n",
    "    df_publist_alltime.select(\n",
    "        f.col(\"auid\"), f.col(\"eid\").alias(\"past_eid\"), f.col(\"year\").alias(\"past_year\")\n",
    "    ),\n",
    "    [\"auid\"],\n",
    ").filter(\"year - past_year <= 5 and year - past_year > 0 and eid != past_eid\")\n",
    "\n",
    "df_compare = (\n",
    "    df_compare1.unionByName(df_compare2)\n",
    "    .withColumnRenamed(\"auid\", 'self_auid')\n",
    "    # get all authors\n",
    "    .join(df_ani_au.select(f.col(\"auid\"), f.col(\"eid\")), [\"eid\"])\n",
    "    .join(\n",
    "        df_ani_au.select(\n",
    "            f.col(\"auid\").alias(\"past_auid\"), f.col(\"eid\").alias(\"past_eid\")\n",
    "        ),\n",
    "        [\"past_eid\"],\n",
    "    )\n",
    "    # remove self\n",
    "    .filter(\"auid != self_auid and past_auid != self_auid\")\n",
    "    .drop(\"self_auid\")\n",
    "    .drop_duplicates()\n",
    "    .withColumn(\"is_same\", f.when(f.col(\"auid\") == f.col(\"past_auid\"), 1).otherwise(0))\n",
    "    # aggregate\n",
    "    .groupby(\"PersonId\", 'eid', \"auid\")\n",
    "    .agg(f.sum('is_same').alias('past_freq'))\n",
    ")\n",
    "\n",
    "# past freq 0 authors\n",
    "df_0_past_freq = df_compare.filter(\"past_freq == 0\").groupby(\"PersonId\", 'eid').agg(f.countDistinct('auid').alias('fresh_collab_num'))\n",
    "# all collaborator\n",
    "df_all = df_compare.groupby(\"PersonId\", 'eid').agg(f.countDistinct('auid').alias('collab_num'))\n",
    "df_fresh_collab = df_all.join(df_0_past_freq, [\"PersonId\", 'eid'], 'left').fillna(0).withColumn('fresh_perc', f.col('fresh_collab_num') / f.col('collab_num'))\n",
    "\n",
    "df_fresh_collab.toPandas().to_csv(f'/dbfs/path/hbcu/hbcu_collab_fresh.csv')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1091857,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "mobility_publication_match_control",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
